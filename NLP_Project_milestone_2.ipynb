{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_Project_milestone_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kathariemer/DiscreteEventSimulation_PopulationModel/blob/main/NLP_Project_milestone_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsSbxq4IXtbW"
      },
      "source": [
        "# Milestone 2: Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIzlqoqZkoVP"
      },
      "source": [
        "### Data Fetching and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMkHQ7gCVa-w",
        "outputId": "fd07b261-7b15-470b-9699-4bb720a2ad4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir(\"./data\"):\n",
        "    os.mkdir(\"./data\")\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "u = urllib.request.URLopener()\n",
        "u.retrieve(\n",
        "    \"https://raw.githubusercontent.com/CrowdTruth/Medical-Relation-Extraction/master/ground_truth_cause.csv\",\n",
        "    \"data/ground_truth_cause.csv\",\n",
        ")\n",
        "u.retrieve(\n",
        "    \"https://raw.githubusercontent.com/CrowdTruth/Medical-Relation-Extraction/master/ground_truth_treat.csv\",\n",
        "    \"data/ground_truth_treat.csv\",\n",
        ")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('data/ground_truth_treat.csv', <http.client.HTTPMessage at 0x7fe19f4daa90>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKmvjn9bY8Y-"
      },
      "source": [
        "We repeat the same preprocessing steps as in milestone 1, i.e. we replace the terms in the sentences with the strings \"term1\" and \"term2\" respectively, and compute a numeric label for the cause/treat relationship by taking the expert label wherever available and in the other cases taking the crowd labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV_T4G2ClG7y"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def get_term(sentence, start, end):\n",
        "    \"\"\" helper function, which returns the entire term that should be replaced \"\"\"\n",
        "    match = re.search(\"[^\\w]\", sentence[end:])\n",
        "    true_end = end + match.start() if match else end\n",
        "    return(sentence[start: true_end])\n",
        "\n",
        "def replace_terms(df):\n",
        "  for i in range(0,len(cause)): #change the terms term1 and term2 in each sentence to \"term1\" and \"term2\"\n",
        "    row = df.iloc[i]\n",
        "    sentence = row[\"sentence\"]\n",
        "    term1 = get_term(sentence, row.b1, row.e1)\n",
        "    term2 = get_term(sentence, row.b2, row.e2)\n",
        "    sentence = sentence.replace(term1,'term1').replace(term2,'term2 ')\n",
        "    df.at[i, 'sentence'] = sentence\n",
        "\n",
        "\n",
        "def extract_labels(df):\n",
        "    expert= df.expert\n",
        "\n",
        "    crowd = df.crowd\n",
        "    label = 0 #default label if no other label\n",
        "\n",
        "    if expert == 1:\n",
        "        label = 1 \n",
        "    elif pd.isnull(expert) and crowd > 0:\n",
        "        label = 1  \n",
        "    \n",
        "    return label"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn28cFXqrCs_"
      },
      "source": [
        "Here we apply the preprocessing to the cause-dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w85O_amzqd-c",
        "outputId": "058e3c07-e79a-47f8-cfb5-4205280bec4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "#load cause data\n",
        "cause = pd.read_csv(\"data/ground_truth_cause.csv\")\n",
        "replace_terms(cause)\n",
        "\n",
        "cause[\"label\"] = cause.apply(extract_labels, axis=1) \n",
        "cause[[\"SID\", \"sentence\", \"label\"]].head()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100003</td>\n",
              "      <td>The limited data suggest that, in children wit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100039</td>\n",
              "      <td>term1 are associated with difficult behaviors ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100079</td>\n",
              "      <td>The term term1 is employed to indicate ataxia ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100086</td>\n",
              "      <td>Non hereditary causes of cerebellar degenerati...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100145</td>\n",
              "      <td>The disorder can present with a migratory ture...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      SID                                           sentence  label\n",
              "0  100003  The limited data suggest that, in children wit...      0\n",
              "1  100039  term1 are associated with difficult behaviors ...      0\n",
              "2  100079  The term term1 is employed to indicate ataxia ...      1\n",
              "3  100086  Non hereditary causes of cerebellar degenerati...      1\n",
              "4  100145  The disorder can present with a migratory ture...      0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiiSf9AKvtAl"
      },
      "source": [
        "And here we do the same for the treat dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1jRjsdXqfrL",
        "outputId": "024b98fa-33c9-420c-94cc-29b502284562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "#load treat data\n",
        "treat = pd.read_csv(\"data/ground_truth_treat.csv\")\n",
        "replace_terms(treat)\n",
        "treat[\"label\"] = treat.apply(extract_labels, axis=1)\n",
        "treat[[\"SID\", \"sentence\", \"label\"]].head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100003</td>\n",
              "      <td>The limited data suggest that, in children wit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100039</td>\n",
              "      <td>term1 are associated with difficult behaviors ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100079</td>\n",
              "      <td>The term term1 is employed to indicate ataxia ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100086</td>\n",
              "      <td>Non hereditary causes of cerebellar degenerati...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100145</td>\n",
              "      <td>The disorder can present with a migratory ture...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      SID                                           sentence  label\n",
              "0  100003  The limited data suggest that, in children wit...      0\n",
              "1  100039  term1 are associated with difficult behaviors ...      0\n",
              "2  100079  The term term1 is employed to indicate ataxia ...      0\n",
              "3  100086  Non hereditary causes of cerebellar degenerati...      0\n",
              "4  100145  The disorder can present with a migratory ture...      0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4B1zDhKgTEN"
      },
      "source": [
        "## Split the datasets into training and validation sets\n",
        "\n",
        "As in the practical lecture we will reserve split the data 70% training dat and 30% validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXzUnKoyhQxR"
      },
      "source": [
        "import torch\n",
        "\n",
        "SEED = 1234\n",
        "TEST_SIZE=0.3\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAj7yHngtc7l"
      },
      "source": [
        "from sklearn.model_selection import train_test_split as split"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "423C-3XWJOfr"
      },
      "source": [
        "tr_cause, val_cause = split(cause, test_size=TEST_SIZE, random_state=SEED)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjlc_V2WJQfk"
      },
      "source": [
        "tr_treat, val_treat = split(treat, test_size=TEST_SIZE, random_state=SEED)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz1zr6HWEVdB"
      },
      "source": [
        "## Vector encoding\n",
        "\n",
        "Next we will use the same steps presented in the lecture to convert the text into one-hot encoded vectors.\n",
        "\n",
        "First we will map the text - after some additionally preprocessing using *nltk* - to count vectors, using *CountVectorizer*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4tX8zSZikiA",
        "outputId": "210d79c5-a22e-4cf2-fd6b-fea61578f989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "\n",
        "    def __call__(self, articles):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
        "\n",
        "\n",
        "def prepare_vectorizer(tr_data):\n",
        "    vectorizer = CountVectorizer(\n",
        "        max_features=3000, tokenizer=LemmaTokenizer(), stop_words=\"english\"\n",
        "    )\n",
        "\n",
        "    word_to_ix = vectorizer.fit(tr_data.sentence)\n",
        "\n",
        "    return word_to_ix"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVodJvxjjo1j"
      },
      "source": [
        "Now let us map the text in the *cause* data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qYxEa7DiuCH",
        "outputId": "21b3ab8b-7641-4cc2-e9e3-dfd8fd26d0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cause_words_to_ix = prepare_vectorizer(tr_cause)\n",
        "CAUSE_VOC_SIZE = len(cause_words_to_ix.vocabulary_)\n",
        "assert CAUSE_VOC_SIZE == 3000"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyPob_95j01q"
      },
      "source": [
        "As well as the text in the *treat* data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAO31kMckDpw",
        "outputId": "239e670a-77bf-48ad-9983-e5ea52477527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "treat_words_to_ix = prepare_vectorizer(tr_treat)\n",
        "TREAT_VOC_SIZE = len(treat_words_to_ix.vocabulary_)\n",
        "assert TREAT_VOC_SIZE == 3000"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvOczDN4FfyH"
      },
      "source": [
        "As advised in the lecture, next we make sure that all the arrays are on the same device:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXhG9PVCFVoy"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFGzS3PMkUMq"
      },
      "source": [
        "In the next step we will map the data with the help of the respective count vectors to one-hot encoded vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsgqWp0l71N2"
      },
      "source": [
        "def prepare_dataloader(tr_data, val_data, word_to_ix):\n",
        "    tr_data_vecs = torch.FloatTensor(word_to_ix.transform(tr_data.sentence).toarray()).to(\n",
        "        device\n",
        "    )\n",
        "    tr_labels = torch.LongTensor(tr_data.label.tolist()).to(device)\n",
        "\n",
        "    val_data_vecs = torch.FloatTensor(\n",
        "        word_to_ix.transform(val_data.sentence).toarray()\n",
        "    ).to(device)\n",
        "    val_labels = torch.LongTensor(val_data.label.tolist()).to(device)\n",
        "\n",
        "    tr_data_loader = [(sample, label) for sample, label in zip(tr_data_vecs, tr_labels)]\n",
        "    val_data_loader = [\n",
        "        (sample, label) for sample, label in zip(val_data_vecs, val_labels)\n",
        "    ]\n",
        "\n",
        "    return tr_data_loader, val_data_loader"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxKrvpkYkxnM"
      },
      "source": [
        "Let us apply this to our *cause* data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0uSorFu8FOv"
      },
      "source": [
        "cause_tr_loader, cause_val_loader = prepare_dataloader(tr_cause, val_cause, cause_words_to_ix)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp2JoEbElARJ"
      },
      "source": [
        "As well as the *treat* data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTX1cXX7J1Ei"
      },
      "source": [
        "treat_tr_loader, treat_val_loader = prepare_dataloader(tr_treat, val_treat, treat_words_to_ix)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmiYuySDldd8"
      },
      "source": [
        "Then we initialize *DataLoader* objects for both lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmqrRcBs8iuH"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_dataloader_iterators(tr_data_loader, val_data_loader, BATCH_SIZE):\n",
        "    train_iterator = DataLoader(\n",
        "        tr_data_loader,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    valid_iterator = DataLoader(\n",
        "        val_data_loader,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_iterator, valid_iterator"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j04opIa9l6Zj"
      },
      "source": [
        "We set the batch size and initialize the *DataLoader* object for the vectors, we computed from the *cause* and *treat* data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcPAvvCsHASc"
      },
      "source": [
        "# Batch size may be modified\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rArIZTDG8tE4"
      },
      "source": [
        "cause_train_iterator, cause_valid_iterator = create_dataloader_iterators(\n",
        "    cause_tr_loader, cause_val_loader, BATCH_SIZE\n",
        ")\n",
        "assert type(cause_train_iterator) == torch.utils.data.dataloader.DataLoader"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZPi5PpXG5Q5"
      },
      "source": [
        "treat_train_iterator, treat_valid_iterator = create_dataloader_iterators(\n",
        "    treat_tr_loader, treat_val_loader, BATCH_SIZE\n",
        ")\n",
        "assert type(treat_train_iterator) == torch.utils.data.dataloader.DataLoader"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zp_odGUmVOB"
      },
      "source": [
        "# Building a neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f8vmLdh9Qix"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class BoWDeepClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, vocab_size, hidden_size):\n",
        "        super(BoWDeepClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(vocab_size, num_labels)\n",
        "\n",
        "        \"\"\" \n",
        "        Below is the extension of the neural network to more layers\n",
        "        but it didn't quite work\n",
        "        \"\"\"\n",
        "        # First linear layer\n",
        "        #self.linear1 = nn.Linear(vocab_size, hidden_size)\n",
        "        ## Non-linear activation function between them\n",
        "        #self.relu = torch.nn.ReLU()\n",
        "        ## Second layer\n",
        "        # self.linear2 = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, bow_vec, sequence_lens):\n",
        "        output = self.linear(bow_vec)\n",
        "        \n",
        "        \"\"\" \n",
        "        Below is the extension of the neural network to more layers\n",
        "        but it didn't quite work\n",
        "        \"\"\"\n",
        "        ## Run the input vector through every layer\n",
        "        #output = self.linear1(bow_vec)\n",
        "        # output = self.relu(output)\n",
        "        # output = self.linear2(output)\n",
        "\n",
        "        # Get the probabilities\n",
        "        return F.log_softmax(output, dim=1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtID2EEz9Vp9"
      },
      "source": [
        "# Size of intermediate representation between linear layers\n",
        "HIDDEN_SIZE = 200\n",
        "# We have only 2 output classes\n",
        "OUTPUT_DIM = 2\n",
        "LEARNING_RATE = 0.001"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dKJcUR0nCdR"
      },
      "source": [
        "CAUSE_INPUT_DIM = CAUSE_VOC_SIZE\n",
        "cause_model = BoWDeepClassifier(OUTPUT_DIM, CAUSE_INPUT_DIM, HIDDEN_SIZE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z49O8yGdnCOL"
      },
      "source": [
        "TREAT_INPUT_DIM = TREAT_VOC_SIZE\n",
        "treat_model = BoWDeepClassifier(OUTPUT_DIM, TREAT_INPUT_DIM, HIDDEN_SIZE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj4R2MfhnbdD"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqBoQpVvwwx8",
        "outputId": "33439351-00a9-4c1d-9216-d38c79b05a1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tr_cause.groupby(\"label\").size()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    1794\n",
              "1     994\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKuE38hp--pv"
      },
      "source": [
        "cause_optimizer = optim.Adam(cause_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Handling class imbalance\n",
        "cause_weights = torch.Tensor([1, 2])\n",
        "cause_criterion = nn.NLLLoss(weight=cause_weights)\n",
        "\n",
        "cause_model = cause_model.to(device)\n",
        "cause_criterion = cause_criterion.to(device)\n",
        "\n",
        "assert cause_model.linear.out_features == 2"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6wnvhiGymKX",
        "outputId": "d6d3428d-3a76-434b-e8a6-fc6d3fc134b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tr_treat.groupby(\"label\").size()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    1793\n",
              "1     995\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2DUcrhZ_XPi"
      },
      "source": [
        "treat_optimizer = optim.Adam(treat_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Handling class imbalance\n",
        "treat_weights = torch.Tensor([1, 2])\n",
        "treat_criterion = nn.NLLLoss(weight=treat_weights)\n",
        "\n",
        "treat_model = treat_model.to(device)\n",
        "treat_criterion = treat_criterion.to(device)\n",
        "\n",
        "assert treat_model.linear.out_features == 2"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROZZTB7mnzzD"
      },
      "source": [
        "# Training and evaluating the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF3byu46_vnO"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "\n",
        "def calculate_performance(preds, y):\n",
        "    \"\"\"\n",
        "    Returns precision, recall, fscore per batch\n",
        "    \"\"\"\n",
        "    # Get the predicted label from the probabilities\n",
        "    rounded_preds = preds.argmax(1)\n",
        "\n",
        "    # Calculate the correct predictions batch-wise and calculate precision, recall, and fscore\n",
        "    # WARNING: Tensors here could be on the GPU, so make sure to copy everything to CPU\n",
        "    precision, recall, fscore, support = precision_recall_fscore_support(\n",
        "        rounded_preds.cpu(), y.cpu()\n",
        "    )\n",
        "\n",
        "    return precision[1], recall[1], fscore[1]\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    # We will calculate loss and accuracy epoch-wise based on average batch accuracy\n",
        "    epoch_loss = 0\n",
        "    epoch_prec = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_fscore = 0\n",
        "\n",
        "    # You always need to set your model to training mode\n",
        "    # If you don't set your model to training mode the error won't propagate back to the weights\n",
        "    model.train()\n",
        "\n",
        "    # We calculate the error on batches so the iterator will return matrices with shape [BATCH_SIZE, VOCAB_SIZE]\n",
        "    for batch in iterator:\n",
        "        text_vecs = batch[0]\n",
        "        labels = batch[1]\n",
        "        sen_lens = []\n",
        "        texts = []\n",
        "\n",
        "        # This is for later!\n",
        "        if len(batch) > 2:\n",
        "            sen_lens = batch[2]\n",
        "            texts = batch[3]\n",
        "\n",
        "        # We reset the gradients from the last step, so the loss will be calculated correctly (and not added together)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # This runs the forward function on your model (you don't need to call it directly)\n",
        "        predictions = model(text_vecs, sen_lens)\n",
        "\n",
        "        # Calculate the loss and the accuracy on the predictions (the predictions are log probabilities, remember!)\n",
        "        loss = criterion(predictions, labels)\n",
        "\n",
        "        prec, recall, fscore = calculate_performance(predictions, labels)\n",
        "\n",
        "        # Propagate the error back on the model (this means changing the initial weights in your model)\n",
        "        # Calculate gradients on parameters that requries grad\n",
        "        loss.backward()\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # We add batch-wise loss to the epoch-wise loss\n",
        "        epoch_loss += loss.item()\n",
        "        # We also do the same with the scores\n",
        "        epoch_prec += prec.item()\n",
        "        epoch_recall += recall.item()\n",
        "        epoch_fscore += fscore.item()\n",
        "    return (\n",
        "        epoch_loss / len(iterator),\n",
        "        epoch_prec / len(iterator),\n",
        "        epoch_recall / len(iterator),\n",
        "        epoch_fscore / len(iterator),\n",
        "    )\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_prec = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_fscore = 0\n",
        "    # On the validation dataset we don't want training so we need to set the model on evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Also tell Pytorch to not propagate any error backwards in the model or calculate gradients\n",
        "    # This is needed when you only want to make predictions and use your model in inference mode!\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # The remaining part is the same with the difference of not using the optimizer to backpropagation\n",
        "        for batch in iterator:\n",
        "            text_vecs = batch[0]\n",
        "            labels = batch[1]\n",
        "            sen_lens = []\n",
        "            texts = []\n",
        "\n",
        "            if len(batch) > 2:\n",
        "                sen_lens = batch[2]\n",
        "                texts = batch[3]\n",
        "\n",
        "            predictions = model(text_vecs, sen_lens)\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            prec, recall, fscore = calculate_performance(predictions, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_prec += prec.item()\n",
        "            epoch_recall += recall.item()\n",
        "            epoch_fscore += fscore.item()\n",
        "\n",
        "    # Return averaged loss on the whole epoch!\n",
        "    return (\n",
        "        epoch_loss / len(iterator),\n",
        "        epoch_prec / len(iterator),\n",
        "        epoch_recall / len(iterator),\n",
        "        epoch_fscore / len(iterator),\n",
        "    )\n",
        "\n",
        "# This is just for measuring training time!\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def training_loop(model, train_iterator, valid_iterator, optimizer, criterion, epoch_number=15):\n",
        "    # Set an EPOCH number!\n",
        "    N_EPOCHS = epoch_number\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "\n",
        "    # We loop forward on the epoch number\n",
        "    for epoch in range(N_EPOCHS):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train the model on the training set using the dataloader\n",
        "        train_loss, train_prec, train_rec, train_fscore = train(\n",
        "            model, train_iterator, optimizer, criterion\n",
        "        )\n",
        "        # And validate your model on the validation set\n",
        "        valid_loss, valid_prec, valid_rec, valid_fscore = evaluate(\n",
        "            model, valid_iterator, criterion\n",
        "        )\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        # If we find a better model, we save the weights so later we may want to reload it\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), \"tut1-model.pt\")\n",
        "\n",
        "        print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
        "        print(\n",
        "            f\"\\tTrain Loss: {train_loss:.3f} | Train Prec: {train_prec*100:.2f}% | Train Rec: {train_rec*100:.2f}% | Train Fscore: {train_fscore*100:.2f}%\"\n",
        "        )\n",
        "        print(\n",
        "            f\"\\t Val. Loss: {valid_loss:.3f} |  Val Prec: {valid_prec*100:.2f}% | Val Rec: {valid_rec*100:.2f}% | Val Fscore: {valid_fscore*100:.2f}%\"\n",
        "        )"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nr4SB3Jrr5A"
      },
      "source": [
        "EPOCH_NUMBER = 15"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTALNA2Grc3p",
        "outputId": "c0136f55-54d4-4a8a-eb2b-9b1650f35207",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_loop(cause_model, cause_train_iterator, cause_valid_iterator, cause_optimizer, cause_criterion)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.676 | Train Prec: 88.38% | Train Rec: 43.51% | Train Fscore: 57.74%\n",
            "\t Val. Loss: 0.656 |  Val Prec: 88.14% | Val Rec: 50.38% | Val Fscore: 63.41%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.627 | Train Prec: 91.09% | Train Rec: 56.71% | Train Fscore: 69.54%\n",
            "\t Val. Loss: 0.631 |  Val Prec: 88.31% | Val Rec: 51.50% | Val Fscore: 64.45%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.589 | Train Prec: 90.76% | Train Rec: 59.06% | Train Fscore: 71.17%\n",
            "\t Val. Loss: 0.614 |  Val Prec: 84.75% | Val Rec: 53.41% | Val Fscore: 64.97%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.559 | Train Prec: 92.41% | Train Rec: 60.02% | Train Fscore: 72.35%\n",
            "\t Val. Loss: 0.600 |  Val Prec: 85.40% | Val Rec: 53.61% | Val Fscore: 65.33%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.534 | Train Prec: 92.53% | Train Rec: 63.21% | Train Fscore: 74.65%\n",
            "\t Val. Loss: 0.593 |  Val Prec: 83.66% | Val Rec: 55.84% | Val Fscore: 66.43%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.513 | Train Prec: 91.80% | Train Rec: 65.52% | Train Fscore: 76.22%\n",
            "\t Val. Loss: 0.588 |  Val Prec: 83.76% | Val Rec: 55.37% | Val Fscore: 66.12%\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.494 | Train Prec: 93.22% | Train Rec: 65.95% | Train Fscore: 76.86%\n",
            "\t Val. Loss: 0.581 |  Val Prec: 83.54% | Val Rec: 55.52% | Val Fscore: 66.18%\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.478 | Train Prec: 92.90% | Train Rec: 67.43% | Train Fscore: 77.60%\n",
            "\t Val. Loss: 0.579 |  Val Prec: 82.13% | Val Rec: 56.70% | Val Fscore: 66.55%\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.463 | Train Prec: 93.62% | Train Rec: 67.88% | Train Fscore: 78.37%\n",
            "\t Val. Loss: 0.578 |  Val Prec: 82.39% | Val Rec: 56.77% | Val Fscore: 66.71%\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.451 | Train Prec: 93.14% | Train Rec: 68.86% | Train Fscore: 78.93%\n",
            "\t Val. Loss: 0.576 |  Val Prec: 80.75% | Val Rec: 57.46% | Val Fscore: 66.58%\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.438 | Train Prec: 93.56% | Train Rec: 70.91% | Train Fscore: 80.39%\n",
            "\t Val. Loss: 0.574 |  Val Prec: 81.66% | Val Rec: 58.20% | Val Fscore: 67.41%\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.426 | Train Prec: 93.16% | Train Rec: 72.91% | Train Fscore: 81.43%\n",
            "\t Val. Loss: 0.572 |  Val Prec: 80.95% | Val Rec: 58.21% | Val Fscore: 67.14%\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.416 | Train Prec: 94.12% | Train Rec: 71.97% | Train Fscore: 81.27%\n",
            "\t Val. Loss: 0.571 |  Val Prec: 80.76% | Val Rec: 58.05% | Val Fscore: 66.97%\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.406 | Train Prec: 93.70% | Train Rec: 73.52% | Train Fscore: 81.98%\n",
            "\t Val. Loss: 0.571 |  Val Prec: 80.46% | Val Rec: 58.34% | Val Fscore: 67.07%\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.397 | Train Prec: 94.34% | Train Rec: 73.51% | Train Fscore: 82.42%\n",
            "\t Val. Loss: 0.570 |  Val Prec: 80.46% | Val Rec: 58.35% | Val Fscore: 67.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUwq6inLCkI1",
        "outputId": "de056f8f-10e3-4310-94c5-51a69eac1875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_loop(treat_model, treat_train_iterator, treat_valid_iterator, treat_optimizer, treat_criterion)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.658 | Train Prec: 94.09% | Train Rec: 47.26% | Train Fscore: 61.94%\n",
            "\t Val. Loss: 0.629 |  Val Prec: 87.66% | Val Rec: 58.94% | Val Fscore: 70.15%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.584 | Train Prec: 92.68% | Train Rec: 68.03% | Train Fscore: 78.18%\n",
            "\t Val. Loss: 0.582 |  Val Prec: 83.35% | Val Rec: 66.76% | Val Fscore: 73.77%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.532 | Train Prec: 89.59% | Train Rec: 76.70% | Train Fscore: 82.33%\n",
            "\t Val. Loss: 0.549 |  Val Prec: 80.67% | Val Rec: 70.82% | Val Fscore: 75.11%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.493 | Train Prec: 89.65% | Train Rec: 77.91% | Train Fscore: 83.14%\n",
            "\t Val. Loss: 0.524 |  Val Prec: 80.17% | Val Rec: 71.11% | Val Fscore: 75.09%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.460 | Train Prec: 89.78% | Train Rec: 79.54% | Train Fscore: 84.14%\n",
            "\t Val. Loss: 0.504 |  Val Prec: 79.63% | Val Rec: 73.61% | Val Fscore: 76.25%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.435 | Train Prec: 89.67% | Train Rec: 81.18% | Train Fscore: 85.03%\n",
            "\t Val. Loss: 0.489 |  Val Prec: 80.36% | Val Rec: 75.26% | Val Fscore: 77.47%\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.413 | Train Prec: 90.11% | Train Rec: 82.22% | Train Fscore: 85.72%\n",
            "\t Val. Loss: 0.476 |  Val Prec: 80.36% | Val Rec: 75.93% | Val Fscore: 77.80%\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.394 | Train Prec: 91.08% | Train Rec: 82.73% | Train Fscore: 86.48%\n",
            "\t Val. Loss: 0.465 |  Val Prec: 79.92% | Val Rec: 76.36% | Val Fscore: 77.84%\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.377 | Train Prec: 91.13% | Train Rec: 84.27% | Train Fscore: 87.38%\n",
            "\t Val. Loss: 0.457 |  Val Prec: 79.92% | Val Rec: 76.53% | Val Fscore: 77.91%\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.363 | Train Prec: 91.59% | Train Rec: 83.92% | Train Fscore: 87.36%\n",
            "\t Val. Loss: 0.449 |  Val Prec: 79.92% | Val Rec: 76.30% | Val Fscore: 77.79%\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.350 | Train Prec: 91.45% | Train Rec: 84.61% | Train Fscore: 87.69%\n",
            "\t Val. Loss: 0.443 |  Val Prec: 79.66% | Val Rec: 76.65% | Val Fscore: 77.86%\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.338 | Train Prec: 92.11% | Train Rec: 85.43% | Train Fscore: 88.36%\n",
            "\t Val. Loss: 0.438 |  Val Prec: 79.62% | Val Rec: 76.82% | Val Fscore: 77.92%\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.327 | Train Prec: 93.03% | Train Rec: 85.59% | Train Fscore: 88.92%\n",
            "\t Val. Loss: 0.433 |  Val Prec: 80.15% | Val Rec: 76.72% | Val Fscore: 78.11%\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.317 | Train Prec: 92.91% | Train Rec: 85.66% | Train Fscore: 88.88%\n",
            "\t Val. Loss: 0.429 |  Val Prec: 80.15% | Val Rec: 77.34% | Val Fscore: 78.42%\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.308 | Train Prec: 92.69% | Train Rec: 85.99% | Train Fscore: 88.96%\n",
            "\t Val. Loss: 0.425 |  Val Prec: 79.89% | Val Rec: 77.11% | Val Fscore: 78.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsfULfxQB37s"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}